{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebf63af-df75-4348-abfc-986a046ca90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp313-cp313-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from wordcloud) (2.1.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from wordcloud) (11.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from wordcloud) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhishekgantana\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Downloading wordcloud-1.9.4-cp313-cp313-win_amd64.whl (300 kB)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10381872-d7fd-4252-99d8-bf3dd9ceee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abhishekgantana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abhishekgantana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Abhishekgantana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abhishekgantana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abhishekgantana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Data Directory: data\n",
      "üìÇ Outputs Directory: outputs\n",
      "‚úÖ Loaded Tweets: 300 rows, 6 cols\n",
      "‚úÖ Loaded Reviews: 200 rows, 6 cols\n",
      "‚úÖ Loaded News: 250 rows, 5 cols\n",
      "\n",
      "üìä Sample from Tweets:\n",
      "   tweet_id username                                         tweet_text  \\\n",
      "0         1   user_1  Great initiative for sustainability and eco pa...   \n",
      "1         2   user_2   Highly recommend this product ‚Äî excellent value!   \n",
      "2         3   user_3  Love the new product launch! Great innovation ...   \n",
      "\n",
      "   likes  retweets   timestamp  \n",
      "0    553        80  2024-01-01  \n",
      "1    728       220  2024-01-02  \n",
      "2    239       246  2024-01-03  \n",
      "\n",
      "üìä Sample from Reviews:\n",
      "   review_id      product_name  \\\n",
      "0          1      Smartphone X   \n",
      "1          2  Smartwatch Elite   \n",
      "2          3  Smartwatch Elite   \n",
      "\n",
      "                                       review_text  rating verified_purchase  \\\n",
      "0  The display and design are absolutely stunning.       1               Yes   \n",
      "1   Sound quality is average, not worth the price.       3                No   \n",
      "2    After a month of use, still works flawlessly.       5                No   \n",
      "\n",
      "  review_date  \n",
      "0  2024-06-01  \n",
      "1  2024-06-03  \n",
      "2  2024-06-05  \n",
      "\n",
      "üìä Sample from News:\n",
      "   headline_id   source                                      headline_text  \\\n",
      "0            1      BBC    Consumer confidence grows as GDP expands in Q3.   \n",
      "1            2  Reuters  Innovative startups disrupt traditional retail...   \n",
      "2            3  Reuters  Major brand unveils sustainability roadmap for...   \n",
      "\n",
      "   category published_date  \n",
      "0  Business     2024-03-01  \n",
      "1   Finance     2024-03-02  \n",
      "2  Business     2024-03-03  \n",
      "üß© Using column 'tweet_text' for Tweets\n",
      "üßπ Cleaned Tweets ‚Äî stored in memory\n",
      "üß© Using column 'review_text' for Reviews\n",
      "üßπ Cleaned Reviews ‚Äî stored in memory\n",
      "üß© Using column 'headline_text' for News\n",
      "üßπ Cleaned News ‚Äî stored in memory\n",
      "üíæ Saved cleaned file: outputs/cleaned_tweets.csv\n",
      "üíæ Saved cleaned file: outputs/cleaned_reviews.csv\n",
      "üíæ Saved cleaned file: outputs/cleaned_news.csv\n",
      "‚úÖ WordCloud saved: outputs/tweets_wordcloud.png\n",
      "‚úÖ WordCloud saved: outputs/reviews_wordcloud.png\n",
      "‚úÖ WordCloud saved: outputs/news_wordcloud.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishekgantana\\AppData\\Local\\Temp\\ipykernel_19072\\1454664203.py:75: UserWarning: Glyph 128292 (\\N{INPUT SYMBOL FOR LATIN LETTERS}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Abhishekgantana\\AppData\\Local\\Temp\\ipykernel_19072\\1454664203.py:77: UserWarning: Glyph 128292 (\\N{INPUT SYMBOL FOR LATIN LETTERS}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(path, bbox_inches=\"tight\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Frequency plot saved: outputs/tweets_top_words.png\n",
      "‚úÖ Frequency plot saved: outputs/reviews_top_words.png\n",
      "‚úÖ Frequency plot saved: outputs/news_headlines_top_words.png\n",
      "\n",
      "‚úÖ Data preprocessing complete!\n",
      "üìÇ Cleaned files and visuals stored in 'outputs/' folder.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò NOTEBOOK 1: Data Preprocessing\n",
    "# ============================================================\n",
    "# Project: Text, Social Media & Web Analytics Capstone\n",
    "# Author: Abhishek Gantana\n",
    "# Purpose: Load raw text datasets, clean & preprocess text,\n",
    "#          save cleaned versions, and visualize word patterns.\n",
    "# ============================================================\n",
    "\n",
    "# ---------- 1. Library Imports ----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # ‚Üê This one is the new requirement for newer NLTK versions\n",
    "\n",
    "\n",
    "# Download NLTK dependencies (only first run)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ---------- 2. Folder Setup ----------\n",
    "DATA_DIR = \"data\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Data Directory: {DATA_DIR}\")\n",
    "print(f\"üìÇ Outputs Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ---------- 3. Helper Functions ----------\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing URLs, mentions, emojis, stopwords, and punctuation.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove URLs\n",
    "    text = re.sub(r'@\\w+|#', '', text)  # remove mentions and hashtags\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # keep only letters\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def generate_wordcloud(text_series, title, save_name):\n",
    "    \"\"\"Generate and save a word cloud image.\"\"\"\n",
    "    text_combined = \" \".join(text_series.dropna())\n",
    "    wc = WordCloud(width=900, height=500, background_color=\"white\", colormap=\"plasma\").generate(text_combined)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    path = f\"{OUTPUT_DIR}/{save_name}.png\"\n",
    "    plt.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ WordCloud saved: {path}\")\n",
    "\n",
    "def top_word_frequency(text_series, top_n=10, dataset_name=\"Dataset\"):\n",
    "    \"\"\"Display top frequent words.\"\"\"\n",
    "    all_words = \" \".join(text_series).split()\n",
    "    freq = Counter(all_words)\n",
    "    top_words = dict(freq.most_common(top_n))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(top_words.keys(), top_words.values(), color=\"teal\")\n",
    "    plt.title(f\"üî§ Top {top_n} Frequent Words ‚Äî {dataset_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    path = f\"{OUTPUT_DIR}/{dataset_name.lower().replace(' ', '_')}_top_words.png\"\n",
    "    plt.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Frequency plot saved: {path}\")\n",
    "\n",
    "# ---------- 4. Load Datasets ----------\n",
    "files = {\n",
    "    \"Tweets\": f\"{DATA_DIR}/tweets_sample.csv\",\n",
    "    \"Reviews\": f\"{DATA_DIR}/reviews_data.csv\",\n",
    "    \"News\": f\"{DATA_DIR}/news_headlines.csv\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for name, path in files.items():\n",
    "    if os.path.exists(path):\n",
    "        datasets[name] = pd.read_csv(path)\n",
    "        print(f\"‚úÖ Loaded {name}: {datasets[name].shape[0]} rows, {datasets[name].shape[1]} cols\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è File not found: {path}\")\n",
    "\n",
    "# ---------- 5. Preview Data ----------\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nüìä Sample from {name}:\")\n",
    "    print(df.head(3))\n",
    "\n",
    "# ---------- 6. Text Cleaning (Updated & Safe) ----------\n",
    "cleaned_datasets = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    # Try to detect text column automatically\n",
    "    text_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "    \n",
    "    if not text_columns:\n",
    "        print(f\"‚ö†Ô∏è No text column found in {name}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prefer text columns likely named 'text', 'tweet', 'review', or 'headline'\n",
    "    candidates = [col for col in text_columns if any(k in col.lower() for k in ['text', 'tweet', 'review', 'headline', 'content'])]\n",
    "    text_col = candidates[0] if candidates else text_columns[0]\n",
    "\n",
    "    print(f\"üß© Using column '{text_col}' for {name}\")\n",
    "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(clean_text)\n",
    "    \n",
    "    cleaned_datasets[name] = df\n",
    "    print(f\"üßπ Cleaned {name} ‚Äî stored in memory\")\n",
    "\n",
    "\n",
    "# ---------- 7. Save Cleaned Datasets ----------\n",
    "for name, df in cleaned_datasets.items():\n",
    "    file_name = f\"{OUTPUT_DIR}/cleaned_{name.lower()}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"üíæ Saved cleaned file: {file_name}\")\n",
    "\n",
    "# ---------- 8. Generate Word Clouds ----------\n",
    "generate_wordcloud(cleaned_datasets[\"Tweets\"][\"cleaned_text\"], \"Tweets WordCloud\", \"tweets_wordcloud\")\n",
    "generate_wordcloud(cleaned_datasets[\"Reviews\"][\"cleaned_text\"], \"Reviews WordCloud\", \"reviews_wordcloud\")\n",
    "generate_wordcloud(cleaned_datasets[\"News\"][\"cleaned_text\"], \"News Headlines WordCloud\", \"news_wordcloud\")\n",
    "\n",
    "# ---------- 9. Word Frequency Charts ----------\n",
    "top_word_frequency(cleaned_datasets[\"Tweets\"][\"cleaned_text\"], 10, \"Tweets\")\n",
    "top_word_frequency(cleaned_datasets[\"Reviews\"][\"cleaned_text\"], 10, \"Reviews\")\n",
    "top_word_frequency(cleaned_datasets[\"News\"][\"cleaned_text\"], 10, \"News Headlines\")\n",
    "\n",
    "# ---------- 10. Summary ----------\n",
    "print(\"\\n‚úÖ Data preprocessing complete!\")\n",
    "print(\"üìÇ Cleaned files and visuals stored in 'outputs/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f4e56-ffa0-401a-aa6c-7ae81e2a20a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdcfd66-00e9-4144-8b5a-0e4d7234bffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
